
---
title: "torch.fx PTQé‡åŒ–æµç¨‹"
date: "2023-12-01"
author: "NeysaBan"
category: "é‡åŒ–"
tags:
- pytorch
- é‡åŒ–
readTime: "12åˆ†é’Ÿ"
---

- âœ¨Â **Ref**
    
    [TORCH.FXç¬¬äºŒç¯‡â€”â€”PTQé‡åŒ–å®æ“](https://oldpan.me/archives/torch-fx-second-quantize-with-fx)
    
    - ğŸ‘†è®²åˆ°çš„å†…å®¹
        - [ç¬¬ä¸€ç¯‡â€”â€”ä»€ä¹ˆæ˜¯torch.fx](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E7%AC%AC%E4%B8%80%E7%AF%87%E4%BB%80%E4%B9%88%E6%98%AFtorchfx)
        - [ä»€ä¹ˆæ˜¯Torch.FX](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E4%BB%80%E4%B9%88%E6%98%AFtorchfx)
            - [torch.fxä¸é‡åŒ–çš„å…³ç³»](https://www.cnblogs.com/bigoldpan/p/16296035.html#torchfx%E4%B8%8E%E9%87%8F%E5%8C%96%E7%9A%84%E5%85%B3%E7%B3%BB)
            - [ä¸TorchScriptçš„åŒºåˆ«](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E4%B8%8Etorchscript%E7%9A%84%E5%8C%BA%E5%88%AB)
                - [Python to Python?](https://www.cnblogs.com/bigoldpan/p/16296035.html#python-to-python)
                - [FXçš„IRå’ŒJitçš„IR](https://www.cnblogs.com/bigoldpan/p/16296035.html#fx%E7%9A%84ir%E5%92%8Cjit%E7%9A%84ir)
            - [symbolic tracer](https://www.cnblogs.com/bigoldpan/p/16296035.html#symbolic-tracer)
        - [ç›¸å…³ç»“æ„](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E7%9B%B8%E5%85%B3%E7%BB%93%E6%9E%84)
            - [ä¿®æ”¹Graph](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E4%BF%AE%E6%94%B9graph)
            - [ä¼˜é›…åœ°ä¿®æ”¹graphç½‘ç»œ](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E4%BC%98%E9%9B%85%E5%9C%B0%E4%BF%AE%E6%94%B9graph%E7%BD%91%E7%BB%9C)
            - [å€ŸåŠ©replace\_patternæ¥ä¿®æ”¹ç½‘ç»œ](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E5%80%9F%E5%8A%A9replace%5C_pattern%E6%9D%A5%E4%BF%AE%E6%94%B9%E7%BD%91%E7%BB%9C)
            - [Interpreter](https://www.cnblogs.com/bigoldpan/p/16296035.html#interpreter)
            - [Transformer](https://www.cnblogs.com/bigoldpan/p/16296035.html#transformer)
        - [ä¸¾ä¸ªFXçš„æ —å­](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E4%B8%BE%E4%B8%AAfx%E7%9A%84%E6%A0%97%E5%AD%90)
            - [OPèåˆ](https://www.cnblogs.com/bigoldpan/p/16296035.html#op%E8%9E%8D%E5%90%88)
        - [å¦‚ä½•debug](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E5%A6%82%E4%BD%95debug)
            - [ç›´æ¥é€šè¿‡pdbè¿›è¡Œdebug](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E7%9B%B4%E6%8E%A5%E9%80%9A%E8%BF%87pdb%E8%BF%9B%E8%A1%8Cdebug)
            - [æ‰“å°ç”Ÿæˆçš„ä»£ç ï¼Œå¹¶ä¸”å’ŒModuleç»„åˆ](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E6%89%93%E5%8D%B0%E7%94%9F%E6%88%90%E7%9A%84%E4%BB%A3%E7%A0%81%E5%B9%B6%E4%B8%94%E5%92%8Cmodule%E7%BB%84%E5%90%88)
            - [ä½¿ç”¨to\_folderå‡½æ•°](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E4%BD%BF%E7%94%A8to%5C_folder%E5%87%BD%E6%95%B0)
        - [ä¸€äº›é™åˆ¶](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E4%B8%80%E4%BA%9B%E9%99%90%E5%88%B6)
        - [æ’©æˆ‘å§](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E6%92%A9%E6%88%91%E5%90%A7)
        - [å‚è€ƒé“¾æ¥](https://www.cnblogs.com/bigoldpan/p/16296035.html#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5)
    
    [ç”¨æ²ç¥çš„æ–¹æ³•é˜…è¯»PyTorch FXè®ºæ–‡](https://zhuanlan.zhihu.com/p/449908382)
    
    [https://pytorch.org/docs/1.13/quantization.html#quantization-support-matrix:~:text=dynamically quantized operators.-,Quantization Support Matrix,-Quantization Mode Support](https://pytorch.org/docs/1.13/quantization.html#quantization-support-matrix:~:text=dynamically%20quantized%20operators.-,Quantization%20Support%20Matrix,-Quantization%20Mode%20Support)
    
    [PyTorchæ–°æŠ€èƒ½è§£é”ï¼štorch.fx](https://zhuanlan.zhihu.com/p/428735136)
    
    [Quantization Accuracy Debugging â€” PyTorch 2.1 documentation](https://pytorch.org/docs/stable/quantization-accuracy-debugging.html)
    
    é‡åŒ–ç²¾åº¦è°ƒè¯•
    
    [INT8 Quantization for x86 CPU in PyTorch](https://pytorch.org/blog/int8-quantization/)
    
    [æ¨¡å‹å‹ç¼©ï¼šæ¨¡å‹é‡åŒ–æ‰“æ€ªå‡çº§ä¹‹è·¯ - 1 å·¥å…·ç¯‡](https://zhuanlan.zhihu.com/p/355598250)
    
    [PyTorch Numeric Suite Tutorial â€” PyTorch Tutorials 2.1.1+cu121 documentation](https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html)
    
    [FX2TRT-Pytorchè½¬TensorRTæ–°æ–¹å¼-å®è·µtorch.fxç¬¬ä¸‰ç¯‡](https://zhuanlan.zhihu.com/p/580962484)
    
    [Pytorchæ¨¡å‹åŠ é€Ÿç³»åˆ—ï¼ˆä¸€ï¼‰â€”â€”æ–°çš„Torch-TensorRTä»¥åŠTorchScript/FX/dynamo](https://ai.oldpan.me/t/topic/152)
    

## 1 Pytorchä¸­çš„é‡åŒ–æ¡†æ¶

![](https://20231118-1258904223.cos.ap-shanghai.myqcloud.com/image/202602011806207.png)

![](https://20231118-1258904223.cos.ap-shanghai.myqcloud.com/image/202602011806208.png)

äºŒè€…åŒºåˆ«

Eager Mode Quantization æœ‰äº›éœ€è¦è‡ªå·±æ‰‹åŠ¨å»æŒ‡å®šï¼Œæ¯”å¦‚ç®—å­èåˆã€Quant/DeQuant Placementç­‰

ä½†æ˜¯FX Graph Mode Quantizationå°±ä¸ç”¨ï¼Œç„¶è€Œå®ƒçš„ä½œç”¨èŒƒå›´æ˜¯æœ‰é™çš„ï¼Œfx graphé€šè¿‡symbolic traceå¾—åˆ°ï¼Œæœ‰ä¸€å®šçš„traceå¤±è´¥å‡ ç‡ï¼Œè€Œä¸”traceå‡ºçš„fx graphå¯èƒ½æ²¡æœ‰å®Œæ•´è¡¨è¾¾æ¨¡å‹ã€NLPçš„éƒ½ä¸èƒ½ï¼Œç¬¬ä¸€å› ä¸ºè¯­è¨€æ¨¡å‹ä¸­æœ‰å¾ˆå¤šæ§åˆ¶æµç®—å­ if whileç­‰è¿½è¸ªä¸åˆ°ï¼Œç¬¬äºŒå› ä¸ºè¾“å…¥å½¢çŠ¶åŠ¨æ€ï¼›CVåŸºæœ¬éƒ½å¯ä»¥ã€‘ï¼Œæ­¤æ—¶åªæœ‰é€‰æ‹©eager Mode quant

## 2 FX

ä¸ªäººç†è§£ï¼šfxå·¥å…·çš„ä½œç”¨æ˜¯è¯»å–å’Œè½¬æ¢pytorchæ¨¡å‹(symbolic)ï¼Œå¯¹IRï¼ˆIntermediate Representï¼‰è¿›è¡Œè°ƒæ•´ï¼ˆç®—å­èåˆã€å»æ‰ç®—å­ã€æ›¿æ¢ç®—å­ã€æ’å…¥ç®—å­ï¼Œ**è€Œé‡åŒ–ä¹Ÿå¯ä»¥ç†è§£æˆè¿™äº›æ“ä½œçš„ç»„åˆ**ï¼‰ï¼Œè¾“å‡ºï¼ˆPython code generationï¼‰

å®é™…ä¸Šç»™å‡ºçš„åº”ç”¨åœºæ™¯ï¼š

- [Replace one op](https://link.zhihu.com/?target=https%3A//github.com/pytorch/examples/blob/master/fx/replace_op.py)
- [Conv/Batch Norm fusion](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/blob/40cbf342d3c000712da92cfafeaca651b3e0bd3e/torch/fx/experimental/optimization.py%23L50)
- [replace_pattern: Basic usage](https://link.zhihu.com/?target=https%3A//github.com/pytorch/examples/blob/master/fx/subgraph_rewriter_basic_use.py)
- [Quantization](https://link.zhihu.com/?target=https%3A//pytorch.org/docs/master/quantization.html%23prototype-fx-graph-mode-quantization)
- [Invert Transformation](https://link.zhihu.com/?target=https%3A//github.com/pytorch/examples/blob/master/fx/invert.py)
- [feature_extraction](https://link.zhihu.com/?target=https%3A//pytorch.org/vision/stable/feature_extraction.html)

ä¿®æ”¹Nodeçš„å·¥å…·ï¼š

- replace_patternï¼šä¿®æ”¹å®Œä¹‹åéœ€è¦recompileé‡æ–°ç”Ÿæˆforwardä»£ç 

### 2.1 é‡è¦ç»„æˆéƒ¨åˆ† Graph & Graph Module

- torch.fx.Graph
    - torch.fx.Nodeç»„åˆèµ·æ¥ï¼ˆä¸‹é¢æŒ‡ä¸€ä¸ªNodeåŒ…å«å“ªäº›IRï¼‰
        - inputï¼šplaceholder
        - weightï¼šget_attr
        - opï¼š
            - call_function
            - call_module
        - outputï¼šoutput
- GraphModuleï¼šç»§æ‰¿è‡ª`torch.nn.Module`ï¼ŒåŒ…å«äº†å‰å‘forwardå‡½æ•°å’Œç½‘ç»œä¸­æ¨¡å—éœ€è¦çš„**å‚æ•°**ï¼Œè¿™äº›å‚æ•°ä¼šè¢«graphä¸­çš„nodeè°ƒç”¨

### 2.2 é‡åŒ–æµç¨‹(PTQ

- ä»ä»£ç æ¥è®²
    
    [(prototype) FX Graph Mode Post Training Static Quantization â€” PyTorch Tutorials 2.1.1+cu121 documentation](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html?highlight=quantization)
    
    - prepareÂ å‡†å¤‡ï¼šæ’å…¥ Observer/FakeQuantize åŸºäºç”¨æˆ·æŒ‡å®šçš„ qconfig æ¨¡å—
    - calibrate/trainï¼šå–å†³äºè®­ç»ƒåé‡åŒ–æˆ–é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
        - å…è®¸ Observers æ”¶é›†ç»Ÿè®¡æ•°æ®æˆ– FakeQuantize æ¨¡å—æ¥å­¦ä¹ é‡åŒ–å‚æ•°
    - convertï¼šå°†æ ¡å‡†/è®­ç»ƒçš„æ¨¡å‹è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
- ä»åŸç†æ¥è®²
    - fuse ç®—å­
        - conv+bn+relu
        - conv_transpose+bn
        - bn+relu
    - æ’å…¥observerï¼š`insert_observers_for_model()`
        - åœ¨è¯¥å‡½æ•°ä¸­æ£€æŸ¥qconfigæ˜¯å¦åˆæ³•
            - å¦‚æœæœ‰æƒ³è¦é‡åŒ–ä¸”pytorchäº‹å®ä¸Šå¯ä»¥å®ç°ï¼Œä½†æ˜¯åœ¨pytorchç»™å‡ºçš„supportåˆ—è¡¨ä¸­æ²¡æœ‰ï¼Œå¯ä»¥å°è¯•åœ¨è¿™é‡Œæ³¨é‡Šæ‰ç›¸åº”çš„æ£€æŸ¥è¯­å¥
        - æ’å…¥é‡åŒ–è§‚å¯ŸèŠ‚ç‚¹
            - weightï¼šç”±äºä¸éœ€è¦ä½¿ç”¨æ•°æ®æ¥è§‚å¯Ÿï¼Œå› æ­¤ä¸éœ€è¦observers
            - activationï¼šéœ€è¦observers(HistogramObserver)
    - calibrationï¼šæ”¶é›†activationçš„scale & zero_point
    - è½¬æ¢æˆé‡åŒ–åçš„ç®—å­
        - activation(input, output)ï¼š `replace_observer_with_quantize_dequantize()`
        - weight(get_attr)ï¼š `convert_weighted_module()` æ­¤æ—¶æ‰æ”¶é›†scale & zero_pointï¼Œå¯¹åº”çš„observers(PerChannelMinMaxObserver)

### 2.3 debugç²¾åº¦

- å®˜æ–¹æ–‡æ¡£
    
    [Quantization Accuracy Debugging â€” PyTorch 2.1 documentation](https://pytorch.org/docs/stable/quantization-accuracy-debugging.html)
    
    - çµæ•åº¦åˆ†æï¼šNumeric Suite [ğŸª](https://www.zhihu.com/question/489354492/answer/3234519045)
        
        [PyTorch Numeric Suite Tutorial â€” PyTorch Tutorials 2.1.1+cu121 documentation](https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html)
        
        1. **æ¯”è¾ƒæƒé‡çš„é‡åŒ–æŸå¤±**ï¼šå®ƒå¯ä»¥å¸®åŠ©ä½ æ‰¾åˆ°å“ªäº›æƒé‡å—åˆ°é‡åŒ–å½±å“æœ€å¤§ï¼Œä¹Ÿå°±æ˜¯å“ªäº›æƒé‡åœ¨é‡åŒ–åå…·æœ‰è¾ƒå¤§çš„è¯¯å·®ã€‚
        2. **æ¯”è¾ƒæ¿€æ´»çš„ç´¯ç§¯é‡åŒ–æŸå¤±**ï¼šå®ƒå¯ä»¥è¯„ä¼°æ•´ä¸ªæ¨¡å‹çš„æ¿€æ´»åœ¨é‡åŒ–åçš„æ€»ä½“è¯¯å·®ã€‚
        3. **æ¯”è¾ƒæ¯ä¸ªæ“ä½œç¬¦çš„æ¿€æ´»é‡åŒ–æŸå¤±**ï¼šå®ƒå…è®¸ä½ æŸ¥çœ‹æ¯ä¸ªæ“ä½œç¬¦ï¼ˆä¾‹å¦‚å·ç§¯å±‚ã€[æ± åŒ–å±‚](https://www.zhihu.com/search?q=%E6%B1%A0%E5%8C%96%E5%B1%82&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3234519045%7D)ç­‰ï¼‰åœ¨é‡åŒ–åçš„è¯¯å·®æƒ…å†µï¼Œå¸®åŠ©ä½ æ‰¾åˆ°å¯¹é‡åŒ–æœ€æ•æ„Ÿçš„æ“ä½œç¬¦ã€‚
- ä¸€ä¸ªå‚æ•°ï¼šåœ¨é‡åŒ–æ—¶æŒ‡å®š `is_reference` å‚æ•°ï¼Œä¹Ÿå°±æ˜¯æ¨¡æ‹Ÿé‡åŒ–
    - å¦‚æœæ­¤æ—¶ç²¾åº¦éƒ½ä¸ç¬¦åˆè¦æ±‚ï¼šé‚£ä¹ˆç¡¬ä»¶ä¸Šç²¾åº¦ä¸€å®šä¸ç¬¦åˆè¦æ±‚
    - ä½†å¦‚æœæ­¤æ—¶ç²¾åº¦ç¬¦åˆè¦æ±‚ï¼Œä½†çœŸæ­£éƒ¨ç½²åˆ°ç¡¬ä»¶ä¸Šå‡ºç°é—®é¢˜ï¼šè¯´æ˜æ˜¯int8ç®—å­çš„å®ç°é—®é¢˜
- ShapePropï¼šfrom [ğŸ§€](https://oldpan.me/archives/torch-fx-second-quantize-with-fx#:~:text=%E7%94%A8%E4%BA%8E%E4%BD%9C%E5%AF%B9%E6%AF%94%E3%80%82-,DEBUG%20%E7%B2%BE%E5%BA%A6,-%E5%88%A9%E7%94%A8reference%E6%A8%A1%E5%9E%8B)  å¯¹æ¯”ä»¥ä¸‹å‡ ä¸ªæ–¹é¢çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    ```python
    import torch
    import torch.fx
    from torch.fx.node import Node
    
    from typing import Dict
    
    class ShapeProp:
        """
        Shape propagation. This class takes a `GraphModule`.
        Then, its `propagate` method executes the `GraphModule`
        node-by-node with the given arguments. As each operation
        executes, the ShapeProp class stores away the shape and
        element type for the output values of each operation on
        the `shape` and `dtype` attributes of the operation's
        `Node`.
        """
        def __init__(self, mod):
            self.mod = mod
            self.graph = mod.graph
            self.modules = dict(self.mod.named_modules())
    
        def propagate(self, *args):
            args_iter = iter(args)
            env : Dict[str, Node] = {}
    
            def load_arg(a):
                return torch.fx.graph.map_arg(a, lambda n: env[n.name])
    
            def fetch_attr(target : str):
                target_atoms = target.split('.')
                attr_itr = self.mod
                for i, atom in enumerate(target_atoms):
                    if not hasattr(attr_itr, atom):
                        raise RuntimeError(f"Node referenced nonexistant target {'.'.join(target_atoms[:i])}")
                    attr_itr = getattr(attr_itr, atom)
                return attr_itr
    
    		        # ä¸»è¦ä¿®æ”¹ä»¥ä¸‹éƒ¨åˆ†
    		        for node in self.graph.nodes:
    		            # op_simå³å½“å‰çš„node-op
    								result_fp32_layer = op_sim.forward_fp32(*load_arg(node.args), **load_arg(node.kwargs))
    								result_int8_layer = op_sim(*load_arg(node.args), **load_arg(node.kwargs))
    								result_fp32_model = op_sim.forward_fp32(*load_arg_fp32(node.args), **load_arg_fp32(node.kwargs))
    								activation_dif_accmulated = torch_cosine_similarity(result_int8_layer, result_fp32_model)
    								activation_dif_layer = torch_cosine_similarity(result_int8_layer, result_fp32_layer)
    								weight_dif = torch_cosine_similarity(op_sim.weight, op_sim.get_weight())
    
                # This is the only code specific to shape propagation.
                # you can delete this `if` branch and this becomes
                # a generic GraphModule interpreter.
                if isinstance(result, torch.Tensor):
                    node.shape = result.shape
                    node.dtype = result.dtype
    
                env[node.name] = result
    
            return load_arg(self.graph.result)
    ```
    
    - å½“å‰æ¿€æ´»å±‚FP32-INT8è¯¯å·®
    - å½“å‰æ¿€æ´»å±‚FP32-INT8ç´¯è®¡è¯¯å·®
    - å½“å‰å±‚æƒé‡è¯¯å·®