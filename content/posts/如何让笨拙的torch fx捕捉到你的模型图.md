---
title: "å¦‚ä½•è®©ç¬¨æ‹™çš„torch fxæ•æ‰åˆ°ä½ çš„æ¨¡å‹å›¾"
date: "2024-01-19"
author: "NeysaBan"
category: "é‡åŒ–"
tags:
 - pytorch
 - é‡åŒ–
readTime: "12åˆ†é’Ÿ"
---

- âœ¨**Ref**
    
    [](https://pytorch.org/docs/1.10/torch.quantization.html)
    
    [æ·±å…¥ç†è§£ TORCH.FX æ¨¡å—](https://zhuanlan.zhihu.com/p/625690498)
    
    [pytorch2.0ç›®å‰å€¼å¾—æ›´æ–°å—? - çŸ¥ä¹](https://www.zhihu.com/question/596483796/answer/3125767007)
    

<aside>
ğŸ“Œ motivationï¼šåœ¨å®ä¹ çš„æ—¶å€™ï¼Œé’ˆå¯¹æ¨¡å‹é‡åŒ–ä»»åŠ¡ï¼Œéƒ¨ç½²å¹³å°æ˜¯orinï¼Œå¯¹åº”ä½¿ç”¨çš„é‡åŒ–å·¥å…·æ˜¯trtï¼›ç„¶è€Œåé¢è¦æ¢åœ°å¹³çº¿å¾ç¨‹ï¼Œåªèƒ½ç”¨torch.fxï¼Œå› æ­¤è°ƒç ”äº†torch.fxå¦‚ä½•åº”ç”¨åˆ°ç°æœ‰æ¨¡å‹ä¸Šã€‚

</aside>

## 1. torch.fxä¸trtçš„é‡åŒ–å¯¹æ¯”

| å¯¹æ¯”é¡¹ | torch.fx (PyTorch 1.10) | TensorRT | PyTorch 2.x |
| --- | --- | --- | --- |
| æ”¯æŒçš„åç«¯ | both CPU & CUDA | CUDA | /(ä»£è¡¨å’Œ1.10ç›¸åŒ) |
| é‡åŒ–æ¨¡å¼ | é™æ€é‡åŒ–ã€åŠ¨æ€é‡åŒ–ï¼ˆä½†qatä»…æ”¯æŒé™æ€é‡åŒ–ï¼‰ | é™æ€ã€åŠ¨æ€ | / |
| å¯¹ç§°/éå¯¹ç§°é‡åŒ– | `torch.qscheme` â€” æ”¯æŒç±»å‹ï¼š`per_tensor_affine`(éå¯¹ç§°)ã€`per_channel_affine`(éå¯¹ç§°)ã€`per_tensor_symmetric`(å¯¹ç§°)ã€`per_channel_symmetric`(å¯¹ç§°) | åªæœ‰å¯¹ç§°é‡åŒ– | / |
| per-channel/per-tensoré‡åŒ– | activationï¼šper-channelã€per-tensorçš„éå¯¹ç§°é‡åŒ–ï¼›weightï¼šåªæœ‰convå’Œlinearç®—å­æ”¯æŒper-channelï¼Œå…¶ä»–per-tensor | activationï¼šper-tensorï¼›weightï¼šconvã€deconvã€fcã€matmul(éœ€è¦çŸ©é˜µ2Dï¼Œä¸”ç¬¬äºŒä¸ªinputæ’å®šï¼‰æ”¯æŒper-channel | / |
| é€‰æ‹©scaleå’Œzero pointçš„æ–¹å¼ | HistogramObserverã€MinMaxObserverã€MovingAverageMinMaxObserverã€MovingAveragePerChannelMinMaxObserverã€NoopObserverã€ObserverBaseã€PerChannelMinMaxObserverã€RecordingObserver | pytorch_quantization/calibä¸‹ï¼šMaxã€Histogram | / |
| æ”¯æŒçš„æ•°æ®ç±»å‹ | INT8, UINT8, INT32 | FP32, FP16, INT8, INT32, UINT8, BOOL | INT8, UINT8, INT32, FP16 |
| qatæµç¨‹ | PyTorch1.10æ¨¡å‹æ²¡è·‘é€š(onnxå¯¼å‡ºæŠ¥é”™)ã€‚PyTorch1.12æµç¨‹ï¼š1.prepare_qat_fx(qconfig/prepare_custom_config_dict/backend_config_dict) 2.train 3.convert_fx | (yolov5) 1.qdqç®—å­æ’å…¥ 2.calibrate 3.finetune | / |
| ç®—å­æ”¯æŒæƒ…å†µ | [PyTorch 1.10æ–‡æ¡£](https://pytorch.org/docs/1.10/quantization-support.html)ï¼ŒConvTransposeå¯é€šè¿‡ä¿®æ”¹åº•å±‚ä»£ç å¼ºåˆ¶æ”¯æŒ | Conv, ConvTranspose, Linear, LSTM, LSTMCell, AvgPool, AdaptiveAvgPool | [PyTorch 2.1æ–‡æ¡£](https://pytorch.org/docs/2.1/quantization.html) |

**ç»“è®ºï¼š**
- æ‰‹åŠ¨å®ç°çš„ä»£ç é‡ï¼štorch.fxç›¸å¯¹è¾ƒå°‘
- torch.fxçš„ç¬¦å·è¿½è¸ªä»…è¿½è¸ªtorch.nnä¸­çš„å‡½æ•°
  - ä¸æ”¯æŒçš„å‡½æ•°è¦ç”¨ `@torch.fx.wrap` åŒ…èµ·æ¥
  - ä»…ä»…ä½¿ç”¨torch.fxï¼Œæ˜¯ä¸æ”¯æŒåŠ¨æ€æ•°æ®æµ(if-else)çš„ã€ä¸è¿‡è¿™é‡Œyolov5çš„trt qaté‡åŒ–æ˜¯å¦ä¹Ÿæ˜¯å› ä¸ºåŠ¨æ€æ§åˆ¶æµçš„åŸå› ä¸èƒ½ç¡®å®šaddç®—å­çš„scaleï¼Ÿ
- ç‰ˆæœ¬é—®é¢˜ï¼šç›®å‰aiå¹³å°ä¸Šä½¿ç”¨çš„æ˜¯pytorch1.10ï¼Œè¯¥ç‰ˆæœ¬çš„torch.fxå°šä¸æˆç†Ÿï¼Œç¬¦å·è¿½è¸ªå›¾çš„æ–¹å¼ä¹Ÿæœ‰å¾ˆå¤šé—®é¢˜ï¼›pytorch2.0æ¨å‡ºdynamoï¼Œå…è®¸åŠ¨æ€è¿½è¸ªå›¾ï¼Œè¦å¥½å¾ˆå¤š
- æ”¯æŒçš„æ•°æ®ç±»å‹ï¼štorch.fxä¸æ”¯æŒfp16
- scaleçš„ç¡®å®šæ–¹å¼ï¼šçœ‹èµ·æ¥æ˜¯torch.fxæ”¯æŒçš„èŒƒå›´å¹¿ä¸€äº›ã€‚å­˜ç–‘ï¼Œè¿™ä¸ªä¸æ¶‰åŠåˆ°ç¡¬ä»¶çš„è¯ï¼Œé‚£éš¾é“ä¸æ˜¯è‡ªå·±æƒ³per-channelå°±per-channelå—ï¼Ÿä¸ºä»€ä¹ˆè¿˜è¦ä¸“é—¨è¯´æ˜å“ªäº›ç®—å­æ”¯æŒä¸æ”¯æŒï¼Ÿè€Œä¸”pytorchä¸­ä¹Ÿæœ‰è¯´æ³•æ˜¯æ”¯æŒä»€ä¹ˆçœ‹observeré‡Œæ˜¯per-channelè¿˜æ˜¯per-tensor

## 2. ä¸ºä»€ä¹ˆtorch.fxæ•æ‰æ¨¡å‹å›¾æ€»æ˜¯å‡ºé”™

æ¨èé˜…è¯»ï¼š[**pytorch2.0ç›®å‰å€¼å¾—æ›´æ–°å—?**](https://www.zhihu.com/question/596483796/answer/3125767007)

æƒ³è¦ä½¿ç”¨torch.fxé‡åŒ–ï¼Œå°±å¿…é¡»ç»è¿‡torch.nnçš„åŠ¨æ€å›¾è½¬æ¢æˆfxé™æ€å›¾çš„è¿‡ç¨‹ï¼Œç„¶è€Œï¼Œåœ¨è½¬æ¢è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ€»æ˜¯å‡ºé”™ã€‚äº‹å®ä¸Šï¼Œä¸»è¦æ˜¯å› ä¸ºtorch.fxåº•å±‚æ˜¯ç¬¨æ‹™çš„ç¬¦å·è¿½è¸ªã€‚

åœ¨è¿½è¸ªå›¾çš„é—®é¢˜ä¸Šï¼Œæ ¹æ®æ¨èé˜…è¯»çš„å†…å®¹ï¼Œä¸»è¦æœ‰ä»¥ä¸‹ä¸‰ç§éš¾é¢˜ï¼š

1. ä½¿ç”¨äº†æ¡ä»¶åˆ¤æ–­
2. è®¡ç®—ä¸å˜é‡å½¢çŠ¶ç›¸å…³
3. è°ƒç”¨äº†å…¶å®ƒåŒ…ï¼ˆæ¯”å¦‚[numpy](https://www.zhihu.com/search?q=numpy&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3125767007%7D)ã€scipyï¼‰ã€è°ƒç”¨äº†å…¶å®ƒè¯­è¨€çš„æ‰©å±•ï¼ˆæ¯”å¦‚Rustã€C++ï¼‰

è€Œè¿½è¸ªçš„æ–¹æ³•åŠåˆ†æå¦‚ä¸‹ï¼š

| å¯¹æ¯”é¡¹ | ç¬¦å·è¿½è¸ª | å³æ—¶è¿½è¸ª | åŠ¨æ€ä¼˜åŒ– |
| --- | --- | --- | --- |
| å·¥å…· | torch.fxï¼ˆå¾—åˆ°çš„æ˜¯fxå›¾ï¼‰ | torch.jitï¼ˆå¾—åˆ°çš„æ˜¯f_traced.graphï¼‰ | torch.dynamoï¼ˆå¾—åˆ°çš„ä¹Ÿæ˜¯fxå›¾ï¼‰ |
| åŸç† | å‡å®šå‡½æ•°çš„å‚æ•°éƒ½æ˜¯torch.Tensorç±»å‹ï¼ˆåªæ˜¯å°†å®ƒä½œä¸ºä¸€ä¸ªæŠ½è±¡çš„æ•´ä½“ï¼Œè€Œä¸ç¡®å®šå®ƒçš„shapeã€dtypeã€deviceã€requires_gradï¼‰ï¼Œä½¿ç”¨Proxyçš„å˜é‡ä½œä¸ºè¾“å…¥ï¼ˆå‡è®¾ä¸º `x` )ï¼Œè®°å½•åœ¨è¾“å…¥å‚æ•°ä¸Šæ‰§è¡Œçš„å„ç§æ“ä½œ | åœ¨ä»£ç è·‘èµ·æ¥çš„æ—¶å€™ï¼Œæ ¹æ®ç¬¬ä¸€ä¸ªçœŸå®çš„è¾“å…¥æ•°æ®è¿›è¡Œè¿½è¸ªï¼Œå¹¶ä¸”ä¼šè¿½è¸ªé¢„å®šä¹‰ç®—å­çš„è°ƒç”¨ã€‚ | åœ¨ä»£ç è·‘èµ·æ¥çš„æ—¶å€™ï¼Œæ ¹æ®ç¬¬ä¸€ä¸ªçœŸå®çš„è¾“å…¥æ•°æ®è¿›è¡Œè¿½è¸ªã€‚è€Œä¸”ä¼šä½¿ç”¨pythonè§£é‡Šå™¨æä¾›çš„apiï¼ŒåŠ«æŒå…¨éƒ¨çš„å‡½æ•°è°ƒç”¨ï¼Œåˆ†æå­—èŠ‚ç å¹¶ä»ä¸­è·å–è®¡ç®—å›¾åŠè®¾ç½®å®ˆå«æ¡ä»¶ã€‚ |
| è®¡ç®—å›¾éš¾é¢˜ä¸Šçš„è¡¨ç° | 1.error 2.error 3.silent failure | 1.error 2.error 3.silent failure | 1.pass 2.pass 3.passğŸŒŸ |

## 3. torch.fxå›¾æ•è·å¯è¡Œæ–¹æ³•

1. æ¨¡å‹ä¿®æ”¹
    - demo
        
        ```python
        import torch  
        import torch.nn as nn  
        import torch.optim as optim

        from torch.quantization import quantize_fx
          

        input_size = 784  
        hidden_size = 500  
        output_size = 10  
        num_epochs = 5  
        batch_size = 100  
        learning_rate = 0.001  
          
          
        x = torch.randn(batch_size, input_size)  
        y = torch.randint(0, output_size, (batch_size,))  

        @torch.fx.wrap
        def wrap_kwargs(kwargs_dict):
            if 'iter' in kwargs_dict:
                pass

        def wrap_x(x):
            for tensor in x.shape[1]:
                print(tensor)
                break
         
        class Net(nn.Module):  
            def __init__(self, input_size, hidden_size, output_size):  
                super(Net, self).__init__()  
                self.fc1 = nn.Linear(input_size, hidden_size)  
                self.relu = nn.ReLU()  
                self.fc2 = nn.Linear(hidden_size, output_size)
            
            def train(self):
                pass
            def test(self):
                pass
            
            ## general forward
            def forward(self, x, **kwargs):
        
                t = kwargs['iter']
                
                ## --------------------- 1. -----------------
                ## Error: Proxy object cannot be iterated. This can be attempted when the Proxy is used in a loop or as a *args or **kwargs function argument. See the torch.fx docs on pytorch.org for a more detailed explanation of what types of control flow can be traced, and check out the Proxy docstring for help troubleshooting Proxy iteration errors
                if 'iter' in kwargs:
                    pass
        
                ## --------------------- 2. -----------------
                ## TypeError: 'Proxy' object cannot be interpreted as an integer
                for tensor in x.shape[1]:
                    print(tensor)
                    break
                
        
                ## --------------------- 3. -----------------
                ## torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow
                if x:
                    self.train()
                else:
                    self.test()
                
        
                out = self.fc1(x)  
                out = self.relu(out)
                out = self.fc2(out)
                return out
            
            # ## using fx
            # def forward(self, x, kwargs_dict):
        
            #     ## --------------------- 1. -----------------
            #     # if 'iter' in kwargs:
            #     #     pass
                
            #     ## 1.1 factor out the untraceable logic into a top-level function and use fx.wrap on it
            #     wrap_kwargs(kwargs_dict)
        
            #     ## 1.2 read the dict statically
            #     print(kwargs_dict['iter'])
                
            #     ## 1.3 However, if you want to use the contents of the dictionary iteration in the next code, you must explicitly show these in the function parameters.
            #     ## example: kwargs_dict = {'iter': 
            #                                       {'a': xxx, 
            #                                        'b': xxx}
            #                                }
            #    ##           forward(self, x, kwargs_dict)   -> forward(self, x, a, b) 
        
            #     ## --------------------- 2. -----------------
            #     ## factor out the untraceable logic into a top-level function and use fx.wrap on it
            #     wrap_x(x)
                
        
            #     ## --------------------- 3. -----------------
            #     ## retain a fixed logic of the control flow.
            #     self.train()
                
        
            #     out = self.fc1(x)
            #     out = self.relu(out)
            #     out = self.fc2(out)
            #     return out
          
        model = Net(input_size, hidden_size, output_size)

        #------------ new load model param -----------
        # net = torch.load(cfg.model_path, map_location=torch.device('cuda'))
        # model.load_state_dict(net['state_dict'])
        #--------------------- end -------------------

        #------------------ fx qconfig & prepare------------------
        qconfig_dict={"":torch.quantization.get_default_qat_qconfig('qnnpack')} 
        model = quantize_fx.prepare_qat_fx(model, qconfig_dict)
        #-------------------------- end --------------------------

        criterion = nn.CrossEntropyLoss()  
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)  
          

        for epoch in range(num_epochs):  
              
            outputs = model(x)  
            loss = criterion(outputs, y)  
              
              
            optimizer.zero_grad()  
            loss.backward()  
            optimizer.step()  
              
            if (epoch+1) % 1 == 0:  
                print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

        #------------------ fx convert & export ------------------
        model_quantized=quantize_fx.convert_fx(model)
        torch.save(model_quantized.state_dict(), 'fx_model.pth')
        #-------------------------- end --------------------------
        ```
        
    - å‰åæ¨¡å‹ç»“æ„å¯¹æ¯”
        
        ```python
        ## before
        Net(
          (fc1): Linear(in_features=784, out_features=500, bias=True)
          (relu): ReLU()
          (fc2): Linear(in_features=500, out_features=10, bias=True)
        )

        ## after
        print(model)
        GraphModule(
          (x_activation_post_process_0): FusedMovingAvgObsFakeQuantize(
            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (fc1): LinearReLU(
            in_features=784, out_features=500, bias=True
            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(
              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
          (fc1_activation_post_process_0): FusedMovingAvgObsFakeQuantize(
            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (fc2): Linear(
            in_features=500, out_features=10, bias=True
            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(
              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
          (fc2_activation_post_process_0): FusedMovingAvgObsFakeQuantize(
            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )

        torch.fx._symbolic_trace.wrap("__main___wrap_kwargs")
        torch.fx._symbolic_trace.wrap("__main___wrap_x")

        def forward(self, x, kwargs_dict):
            x_activation_post_process_0 = self.x_activation_post_process_0(x)
            wrap_kwargs = __main___wrap_kwargs(kwargs_dict)
            getitem = kwargs_dict['iter'];  kwargs_dict = None
            wrap_x = __main___wrap_x(x);  x = None
            fc1 = self.fc1(x_activation_post_process_0);  x_activation_post_process_0 = None
            fc1_activation_post_process_0 = self.fc1_activation_post_process_0(fc1);  fc1 = None
            fc2 = self.fc2(fc1_activation_post_process_0);  fc1_activation_post_process_0 = None
            fc2_activation_post_process_0 = self.fc2_activation_post_process_0(fc2);  fc2 = None
            return fc2_activation_post_process_0
        ```
        
2. mmrazorï¼šä¸å¯è¡Œï¼Œè¿™ä¸ªåº”è¯¥åªèƒ½è‡ªåŠ¨åŒ…è£…ä¸å¯è¿½è¸ªçš„å‡½æ•°ï¼Œè€Œä¸”è¿˜éœ€è¦è‡ªå·±æŠŠå¯è¿½è¸ªå’Œä¸å¯è¿½è¸ªçš„é€»è¾‘åˆ†å¼€
3. åœ°å¹³çº¿dockerï¼šä¹Ÿæ˜¯æŠŠtorch.fxçš„æ–¹æ³•åŒ…èµ·æ¥äº†ï¼Œä½†æ˜¯åº”è¯¥ä¹Ÿä¸èƒ½å¯¹**kwargsè¿™ç§åšå‡ºè‡ªåŠ¨å¤„ç†ï¼ˆæ²¡æœ‰è¿›ä¸€æ­¥è°ƒç ”ï¼‰
4. torch.dynamoï¼štorch.dynamoè½¬æ¢æˆfxå›¾ï¼Œå†æ¥torch.fxè¿›è¡Œé‡åŒ–ã€‚ä½†æ˜¯éœ€è¦pytorchç‰ˆæœ¬å‡çº§åˆ°2.0
